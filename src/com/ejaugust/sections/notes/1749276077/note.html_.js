return `<p>An incredible combinatorial explosion appears when you try to compute the number of unique URLs that can exist. In fact, <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986">RFC 3986</a> doesn't impose any finite limit on the length of a valid URL. So, in theory, the real number might be countably infinite. Yet, there are physical limitations that make the real number finite in practice. The purpose of this note is not to compute that exact finite number, but to give you a tiny hint at how unfathomably large that number might be, even given the strictest limitations.</p>
<p>There are far, far, far more possible URLs than there are <a ${_.pointAttr()} href=${notes[1762062190].canonicalURL}>particles in the known universe</a>. To compute the "exact" cardinality we'd need to settle a lot of questions, iterate over every applicable protocol, and learn every one of those protocols' unique rules. Then we'd have to settle questions. In this note, I've gone factor-by-factor and, at each step, I've chosen the simplest possible case in order to reduce the set down into one whose size we can compute with a simple exponent.</p>
<h2>Worst-Case Length Limit</h2>
<p>Do we limit our URLs to <a ${_.pointAttr()} class=external href="https://support.microsoft.com/en-us/topic/maximum-url-length-is-2-083-characters-in-internet-explorer-174e7c8a-6666-f4e0-6fd6-908b53c12246#:~:text=Microsoft%20Internet%20Explorer%20has%20a,request%20and%20GET%20request%20URLs.">two thousand characters like Internet Explorer</a> or <a ${_.pointAttr()} class=external href="https://chromium.googlesource.com/chromium/src/+/main/docs/security/url_display_guidelines/url_display_guidelines.md#:~:text=Chrome%20limits%20URLs%20to%20a,is%20used%20on%20VR%20platforms.">two million like Chrome</a>? To answer that, its worth considering that many servers, social media platforms and databases impose limits of their own on shared link size. For the most part, <math display="inline"><mn>2000</mn></math> characters seems to be the worst-case length limit.</p>
<p>Yet, since some modern browsers like Chrome really can process much longer URLs, its worth noting that imposing this limit just removed a <em>lot</em> of potential URLs from the "true" set. In fact, we just removed <math display="inline"><mn>99</mn><mo>.</mo><munder><mrow><mn>999…999</mn></mrow><mrow><munder><mo stretchy="true" symmetric="false">⏟</mo><mtext>≈ 3.9 million 9s</mtext></munder></mrow></munder><mo>%</mo></math>. However, there's still a <em>lot</em> of URLs (and a lot of nuance) left in our miniscule <math display="inline"><mn>0</mn><mo>.</mo><munder><mrow><mn>000…000</mn></mrow><mrow><munder><mo stretchy="true" symmetric="false">⏟</mo><mtext>≈ 3.9 million 0s</mtext></munder></mrow></munder><mn>1</mn><mo>%</mo></math>.</p>
<h2>No Protocol Mutability</h2>
<p>There are <a ${_.pointAttr()} class=external href="https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml">many different</a> <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.1">schemes</a> to choose from. While the IANA registry lists all registered Uniform Resource Identifier (URI) schemes, URLs are technically a subset of URIs that specify a location and access method for a resource (e.g., <code>http</code>, <code>ftp</code>, <code>file</code>, <code>mailto</code>). However, we're going to forget about those and limit ourselves to only <code>https</code>. This leaves us with only a fraction of the original set, but the impact of this is almost nothing compared to the length limit we imposed at the start. Speaking of which, we can go ahead and subtract the 8 characters in <code>"https://"</code> from our <math display="inline"><mn>2000</mn></math>-character limit, leaving us with only <math display="inline"><mn>1992</mn></math> usable characters.</p>
<h2>Worst Possible Host</h2>
<p>What about the <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.2">host</a> (a.k.a. the domain name)? The number of possible domain names is, by itself, vastly larger than the number of particles in the known universe. As a rough estimate, we can look at the maximum length of a domain name (including dots), which is <math display="inline"><mn>253</mn></math> characters. The vast majority of "potential" domain names are <math display="inline"><mn>253</mn></math> characters in length and have as few dots as possible. Each domain label can use the digits 0-9, the lowercase letters a-z and the dash (a total of <math display="inline"><mn>37</mn></math> characters). Although there are some rules about when and how you can use these characters, the impact of those rules is pretty small compared to the impact of the <math display="inline"><mn>253</mn></math>-character length limit. We can estimate that there are roughly <math display="inline"><mo>∼</mo><msup><mn>37</mn><mn>253</mn></msup><mo>≈</mo><msup><mn>10</mn><mn>399</mn></msup></math> domain names.</p>
<p>This estimate covers IPv4 addresses as well as domain names. Yet, the variable length of domain names and the ability to have IPv6 addresses presents us with added complications. Given our length limit, shorter hosts leave us with more space for other URL components whereas longer ones consume that space. Accounting for that in a cardinality computation is far from trivial. Let's get crazy and pick the worst possible domain name and then limit our set to only those URLs that use that domain name:</p>
<pre>this.domain.name.is.made.from.two-hundred-fifty-three.characters.which.means.that.it.is.just.about.as.long.as.it.can.possibly.be.abcdefghijklmnopqrstuvwxyz.it.includes.the.alphabet.just.to.take.up.some.more.letters.abcdefghijklmnopqrstuvwxyz.example.com</pre>
<p>We just removed <math display="inline"><mn>99</mn><mo>.</mo><munder><mrow><mn>999…999</mn></mrow><mrow><munder><mo stretchy="true" symmetric="false">⏟</mo><mtext>almost 400 9s</mtext></munder></mrow></munder><mo>%</mo></math> of the possible hosts. Because the shortest hosts correspond to most <math display=inline><mn>2000</mn></math>-character URLs, the percent of actual URLs we just removed (from our already miniscule set) has even more 9s. Since this domain takes up <math display="inline"><mn>253</mn></math> of our <math display="inline"><mn>1992</mn></math> characters, we are left with only <math display="inline"><mn>1739</mn></math>.</p>
<h2>No Fancy Features</h2>
<p>There's still a lot of unpredictability in what those <math display="inline"><mn>1739</mn></math> characters might be. For example, we could include <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.1">user credentials</a> (a universe of its own), a specific <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.3">port</a> (a number between <math display="inline"><mn>0</mn></math> and <math display="inline"><mn>65535</mn></math>), an untold variety of <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.4">query parameters</a>, and a whole universe of possible <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.5">hash fragments</a>, all of which increase the size of our set, but their impact is actually pretty negligible compared to the impact of our length limit and giant domain name. They also add an incredible amount of complexity to the equation.</p>
<p>They're all optional, so let's just remove them from consideration for today, and only include URLs that have none of these features.</p>
<h2>No Variable-length Pathnames</h2>
<p>At this point, there's only one URL structure we haven't removed from the equation: <a ${_.pointAttr()} class=external href="https://datatracker.ietf.org/doc/html/rfc3986#section-3.3">the pathname</a>. Counting the size of our set is really just counting the number of valid pathnames up to <math display="inline"><mn>1739</mn></math> characters long. There is only one URL in our set that doesn't have a pathname and all other URLs contain at least the initial slash that separates the domain name from the rest of the path, making that first character unusable.</p>
<p>The <a ${_.pointAttr()} href=${notes[1762157702].canonicalURL}>variable-length of pathnames increases our set size a little bit</a> compared to the set with all <math display=inline><mn>1739</mn></math>-character-long pathnames. To find out exactly how much, we'd have to take one (for the empty path) plus one (for the path <code>"/"</code>) plus the number of <math display="inline"><mn>2</mn></math>-character pathnames plus the number of <math display="inline"><mn>3</mn></math>-character ones and so on. That requires a <a ${_.pointAttr()} class=external href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a>. That's too rich for our worst-case set (and the increase it provides is actually negligible as the vast majority of URLs in our set are going to be those whose pathnames that already have exactly <math display="inline"><mn>1739</mn></math> characters). Let's just remove every URL whose pathname is not exactly <math display="inline"><mn>1739</mn></math> characters long.</p>
<p>If we assume about <math display="inline"><mn>81</mn></math> allowed characters in the pathname, we really only removed <math display="inline"><mfrac><mn>1</mn><mn>81</mn></mfrac><mo>≈</mo><mn>1.234567%</mn></math> of our URLs.</p>
<h2>No Special Characters</h2>
<p>I hinted at the fact that modern browsers support <math display="inline"><mn>81</mn></math> characters in a URL pathname. Let's have a look:</p>
<pre>A B C D E F G H I J K L M N O P Q R S T U V W X Y Z\na b c d e f g h i j k l m n o p q r s t u v w x y z\n0 1 2 3 4 5 6 7 8 9\n- . _ ~\n! $ & ' ( ) * + , ; =\n: @\n/\n%</pre>
<p>This makes it seem like it would be possible to use base <math display="inline"><mn>81</mn></math> to compute the cardinality of our remaining set.</p>
<p>There's just one problem: some sequences of these characters are not valid in a URL. Broadly accounting for those invalid sequences (especially the special rules regarding the <code>%</code> symbol) gives us a base closer to <math display="inline"><mn>80</mn></math> meaning we could almost say that our set has about <math display="inline"><msup><mn>80</mn><mn>1739</mn></msup></math> URLs in it.</p>
<p>Yet, despite being valid, some of these characters can trip up URL parsers and platforms that embed links. So, let's lighten our load and remove a bunch more URLs from our set! Let's only consider URLs that use the following <math display="inline"><mn>64</mn></math>-character alphabet to spell out their pathname segments:</p>
<pre>A B C D E F G H I J K L M N O P Q R S T U V W X Y Z\na b c d e f g h i j k l m n o p q r s t u v w x y z\n0 1 2 3 4 5 6 7 8 9\n- _ </pre>
<p>I wish I could tell you that this reduces our set to <math><mfrac><mn>64</mn><mn>80</mn></mfrac></math> of the base-<math display="inline"><mn>80</mn></math> set but that's not how it works on this exponential scale. An estimated <math display="inline"><mn>99</mn><mo>.</mo><munder><mrow><mn>999…999</mn></mrow><mrow><munder><mo stretchy="true" symmetric="false">⏟</mo><mtext>166 9s</mtext></munder></mrow></munder><mo>%</mo></math> of the base-<math display="inline"><mn>80</mn></math> set had at least one of the special characters we removed, leaving us with <math display="inline"><mn>0</mn><mo>.</mo><munder><mrow><mn>000…000</mn></mrow><mrow><munder><mo stretchy="true" symmetric="false">⏟</mo><mtext>165 zeros</mtext></munder></mrow></munder><mn>1</mn><mo>%</mo></math>.</p>
<h2>No Variable-length Segments</h2>
<p>We can't just string together <math display="inline"><mn>0</mn><mo>-</mo><mn>1739</mn></math> base-<math display="inline"><mn>64</mn></math> characters together and call it a day. As mentioned, the first symbol <em>must</em> be a slash. Furthermore, some systems throw an error if it encounters a single pathname segment longer than <math display="inline"><mn>250</mn></math> characters. To get to <math display="inline"><mn>1739</mn></math>, the pathname must have at least <math display="inline"><mn>7</mn></math> slashes. It wouldn't be terribly hard to account for these arrangements but it <em>would</em> mean writing out some geometric series. So, let's only count URLs that have the following exact arrangement: <math display="inline"><mn>7</mn></math> fixed-length segments (<math display="inline"><mn>6</mn><mo>&times;</mo><mn>250</mn></math>-character segments followed by <math display="inline"><mn>1</mn><mo>&times;</mo><mn>232</mn></math>-character segment). This uses exactly <math display="inline"><mn>7</mn></math> slashes, leaving us with <math display="inline"><mn>6</mn><mo>&times;</mo><mn>250</mn><mo>+</mo><mn>1</mn><mo>&times;</mo><mn>232</mn><mo>+</mo><mn>7</mn><mo>=</mo><mn>1739</mn></math> characters, only <math display="inline"><mn>1739</mn><mo>-</mo><mn>7</mn><mo>=</mo><mn>1732</mn></math> of which can actually change from one URL to the next.</p>
<p>Finally, we have a set of completely valid URLs whose exact cardinality can be known. Just for fun, here is a "random" (but valid) URL from that set:</p>
<pre>https://this.domain.name.is.made.from.two-hundred-fifty-three.characters.which.means.that.it.is.just.about.as.long.as.it.can.possibly.be.abcdefghijklmnopqrstuvwxyz.it.includes.the.alphabet.just.to.take.up.some.more.letters.abcdefghijklmnopqrstuvwxyz.example.com/ld9370LC9uzp83Mklv90dujd9v7774n0V7p7em5078B7S790dkb094977au9D07B098yAV76ouyf79J7uft7d5f77g90jki98765dc7hkji7865d7dgy77yuhjkIk8Kgf567hj9ny7GVNjn7677V9jbvr567bj099efbn49efg776ft5x4rxi7T8Yoih0n7hv6r4d657d8g9u8b779ub8yf7t7d75d674s77743s2a1a7772zqzdtcuhbi/007opkolpp7m777ko7n777ou77h8yvtcr7tds5es5241awzsx7dfchvj7jkhboi78g8765c6e4x421s576y57t6fv8iub9ub87ytvc64dc768fv897b9ub9yv7tc7tv8ybhvutF64d52as2zfxcghvbhininIonoi0ih9ubgiI0P1MS07vmllp0097f74d2wa1asswextyvunimlljo00987643212wzqz1a12aswzz23sxerc34dfcrvr/5fvtby65g67buh7nu98m09k0,00k9jubyuhubvtctvubuvtdcrd4dxw23314152375786e6d976f70867f97743d8742a13s7278rf97vtiyfcdtd7xtrwSte7wrxztewzyrTUCtivuyvbOUyv77utrs462s67546rv9vbyv8rtc6X4s13A2ezwrexrcYdexwa2WESDfgtyu789iujhnmko9iujhbVFdre321qasxcfvghuiuytr432was/zxcvfghyui9o876543wsaxcdfghyuI9Oijnjiuygtftdrezwertrytfugyhiu8978gtf7r6d53s64512zwxetCRYtfughioYG87f564dsetrcyvubhij9i8u7t6f5r7743217aqwezsxdcrft7gyh77ujikJHgt5432177777qwertyhj7kJHgre321qaszxcvghjio987654321QWErfcv7bhuyTRfdfgtyui987654321qwerfv77798/bnhjHGv7fghjio09oiujhn7b7VCxsaq12we7rty7u8Ikj7hgFD777seRTyu876YT7r4ewsDFghJiuy6543721Qas7zxcVBnjio9876574321qasDFGhbnJUytrfdsERtyuJHgbvfdeRthBVGCFdsw34543212qaszxcVbghjkio0987654321qasZXcvbnhJKo0987654ERD7sw21qas7ZXcfghyuI9O0okmjki87uyhgbvgt54edsxzsa/Q12qwasXCfrt56tygBnhjio909OIKjmnko09iujhY76tfgcder321q7aszxCvgbhjui7uY6T5r4e321234567879i8UytrfdsXcvbghjIuyTRewqA7SzxcvbgHJik7oKjmjI7UyhgfrEwszx7sAWaszxcdftYUijkmjNHBGfrtYuiuYTredFCghyui89876543eDfgyuytr43edfgyuhbghJUI987654321qaszxCVghui9o876543wErt/fGHYBUtcu63s312asydu6IYUTYVbokuvhtExt42at4wrexcutyvbo8ygoiyrc53yr3254Sdu6rfvoyboiybiutyvyuCtycurc6u5c865777ctycuyut7ityd7i6757DE7U7674s5u74s54sd7xry7txc7jyrfxd7j7yrDujdI7775edu77564st427as13szr7ezUT7rxiYGCkyuC77FJut7ykm79OM0p7978tx9</pre>
<p>It's exactly <math display="inline"><mn>2000</mn></math> characters long. It has a giant (but syntactically valid) domain name. It has a very unlikely (but still valid) pathname. And I mashed my keyboard as fast as I could to make it. It still took a surprisingly long time to type it out, which gives you some idea of just how many URLs of this limited form exist.</p>
<h2>Final Cardinality of our Set</h2>
<p>Before we look at the actual number, let's consider the scale of everything. This set is truly a mindbogglingly miniscule fraction of a fraction of a fraction of a fraction of a fraction of a fraction of a fraction of the set of valid URLs that can actually be used in practice. It's not a drop in the bucket, it's not even a subatomic particle in the bucket. But it <em>is</em> trivial to measure the size of it: <math><mi>k</mi><mo>=</mo><msup><mn>64</mn><mn>1732</mn></msup></math>.</p>
<p>For reference, there are "only" an estimated <math><msup><mn>10</mn><mn>97</mn></msup><mo>≈</mo><msup><mn>64</mn><mn>54</mn></msup></math> particles in the known universe (including photons, neutrinos, and relativistic particles), a number which looks something like this:</p>
<pre>1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</pre>
<p>The ratio of that (<math><msup><mn>64</mn><mn>54</mn></msup></math>) to the size of our mindbogglingly miniscule fraction (<math><msup><mn>64</mn><mn>1732</mn></msup></math>) is basically <math display="inline"><mn>0</mn><mo>:</mo><mn>1</mn></math>. But what does <math><msup><mn>64</mn><mn>1732</mn></msup></math> look like? See for yourself:</p>
<pre>${64n ** 1732n}</pre>
<p>That's a lot of URLs. The <a ${_.pointAttr()} class=external href="https://en.wikipedia.org/wiki/Hartley_entropy">hartley entropy</a> of our set is <math display="inline"><mn>1732</mn><mspace width="0.25em"/><mtext><a ${_.pointAttr()} href=${notes[1753817767].canonicalURL}>charms</a></mtext><mo>=</mo><mn>10392</mn><mspace width="0.25em"/><mtext>bits</mtext><mo>=</mo><mn>1.299</mn><mspace width="0.25em"/><mtext>kilobytes</mtext></math>. This tells us exactly how much information content any one of these URLs conveys.</p>
<h2>Extending to the Larger Set</h2>
<p>Realistically, URLs that are 2MB cannot reliably be shared (even if they might work in a chromium browser). Still, let's imagine that they <i>could</i> be share and let's also imagine that URLs can have between one and <math><mi>2</mi><mo>&times;</mo><msup><mn>10</mn><mn>6</mn></msup><mo>=</mo><mn>2000000</mn></math> data-storing characters and that each of these is one of <math display="inline"><mn>90</mn></math> possible URL-safe symbols. This is a massive oversimplification but it might get us into the ballpark. It also requires a geometric series, which we'll go ahead and compute. This larger set contains <math display="inline"><mrow><mi>k</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>&#x2113;</mi><mo>=</mo><mn>1</mn></mrow><mn>2000000</mn></munderover><msup><mn>90</mn><mi>&#x2113;</mi></msup></mrow></math> URLs, which can also be written as <math display="inline"><mfrac><mrow><msup><mn>90</mn><mn>2000001</mn></msup><mo>&#x2212;</mo><mn>90</mn></mrow><mn>89</mn></mfrac></math>.</p>
<p>This evaluates to approximately <math display="inline"><mrow><mo>&#x2248;</mo><mn>1.05</mn><mo>&#x00D7;</mo><msup><mn>10</mn><mn>3908485</mn></msup></mrow></math>. This is too big to compute and definitely too big to show you and if I did show it to you, it would have over three million digits.</p>
<h2>Conclusion</h2>
<p>Thanks for reading. I realize these estimates might not be as satisfying as knowing the exact number of valid URLs. Computing that number would be a massive undertaking and require more assumptions and math than I'm prepared to do in this humble notebook. Still, I hope that the ones we did count gives you some idea of how stupendously large the real number might be.</p>`